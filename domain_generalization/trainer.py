import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import json
import os
from typing import Dict, List, Optional
from tqdm import tqdm
import logging
from datetime import datetime
import random


class Trainer:
    """ç”¨äºé¢†åŸŸæ³›åŒ–æ¨¡å‹çš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.model_name = 'model'  # é»˜è®¤åç§°ï¼Œå¯ä»¥åç»­è®¾ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # AdaINç›¸å…³é…ç½®
        self.use_adain = config.get('use_adain', True)
        self.adain_prob = config.get('adain_prob', 0.5)  # ä½¿ç”¨AdaINçš„æ¦‚ç‡
        
        # å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ•°æ®ç”¨äºåŸŸéšæœºé‡‡æ ·
        self.domain_data_cache = {}  # {domain_id: [samples]}
        
        # é»˜è®¤loggerï¼Œå¯ä»¥åç»­è¢«æ›¿æ¢
        self.logger = logging.getLogger('trainer')
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            self.logger.addHandler(handler)
        
        # è®­ç»ƒå‚æ•°
        self.learning_rate = config.get('learning_rate', 1e-3)
        self.weight_decay = config.get('weight_decay', 1e-5)
        self.num_epochs = config.get('num_epochs', 100)
        self.early_stopping_patience = config.get('early_stopping_patience', 10)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.learning_rate, 
            weight_decay=self.weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨Huber Lossï¼Œå¯¹å¼‚å¸¸å€¼æ›´ç¨³å®š
        self.criterion = nn.SmoothL1Loss()
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        
        # æ·»åŠ æ¨¡å‹å‚æ•°æ£€æŸ¥
        self._check_model_parameters()
        
        self.logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        self.logger.info(f"å­¦ä¹ ç‡: {self.learning_rate}, æƒé‡è¡°å‡: {self.weight_decay}")
        self.logger.info(f"è®­ç»ƒè½®æ•°: {self.num_epochs}, æ—©åœè€å¿ƒ: {self.early_stopping_patience}")
        self.logger.info(f"AdaINé…ç½®: å¯ç”¨={self.use_adain}, æ¦‚ç‡={self.adain_prob}")
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{self.model_name}_{timestamp}.log"
        log_filepath = os.path.join(self.log_dir, log_filename)
        
        # åˆ›å»ºä¸“ç”¨çš„logger
        self.logger = logging.getLogger(f"trainer_{self.model_name}")
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ä¹‹å‰çš„handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶handler
        file_handler = logging.FileHandler(log_filepath, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—é‡å¤
        self.logger.propagate = False
        
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {log_filepath}")
        
    def _check_model_parameters(self):
        """æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦æœ‰é—®é¢˜"""
        self.logger.info("æ£€æŸ¥æ¨¡å‹å‚æ•°...")
        total_params = 0
        nan_params = 0
        inf_params = 0
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            if torch.isnan(param).any():
                nan_params += torch.isnan(param).sum().item()
                self.logger.warning(f"å‚æ•° {name} åŒ…å« {torch.isnan(param).sum().item()} ä¸ªNaNå€¼")
            if torch.isinf(param).any():
                inf_params += torch.isinf(param).sum().item()
                self.logger.warning(f"å‚æ•° {name} åŒ…å« {torch.isinf(param).sum().item()} ä¸ªæ— ç©·å€¼")
        
        self.logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        self.logger.info(f"NaNå‚æ•°æ•°é‡: {nan_params}")
        self.logger.info(f"æ— ç©·å‚æ•°æ•°é‡: {inf_params}")
        
        if nan_params > 0 or inf_params > 0:
            self.logger.warning("æ¨¡å‹å‚æ•°åŒ…å«å¼‚å¸¸å€¼ï¼Œé‡æ–°åˆå§‹åŒ–...")
            self._reinitialize_model()
    
    def _reinitialize_model(self):
        """é‡æ–°åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        for module in self.model.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        nn.init.constant_(param.data, 0)
        
        self.logger.info("æ¨¡å‹å‚æ•°é‡æ–°åˆå§‹åŒ–å®Œæˆ")
        
    def _build_domain_cache(self, dataloader):
        """æ„å»ºåŸŸæ•°æ®ç¼“å­˜ï¼Œç”¨äºéšæœºé‡‡æ ·"""
        if not self.use_adain:
            return
            
        self.logger.info("æ„å»ºåŸŸæ•°æ®ç¼“å­˜...")
        self.domain_data_cache = {}
        
        for batch in dataloader:
            data = batch['data'].float()
            domains = batch['domain']
            
            for i in range(len(data)):
                domain_id = domains[i].item()
                if domain_id not in self.domain_data_cache:
                    self.domain_data_cache[domain_id] = []
                self.domain_data_cache[domain_id].append(data[i])
        
        # è½¬æ¢ä¸ºtensor
        for domain_id in self.domain_data_cache:
            self.domain_data_cache[domain_id] = torch.stack(self.domain_data_cache[domain_id])
            
        self.logger.info(f"åŸŸæ•°æ®ç¼“å­˜æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(self.domain_data_cache)} ä¸ªåŸŸ")
        for domain_id, samples in self.domain_data_cache.items():
            self.logger.info(f"  åŸŸ {domain_id}: {len(samples)} ä¸ªæ ·æœ¬")
    
    def _get_random_domain_samples(self, batch_size, current_domains=None):
        """ä»æ‰€æœ‰åŸŸä¸­éšæœºé‡‡æ ·æ ·æœ¬ä½œä¸ºstyleè¾“å…¥"""
        if not self.use_adain or not self.domain_data_cache:
            return None
            
        style_samples = []
        available_domains = list(self.domain_data_cache.keys())
        
        for i in range(batch_size):
            # å¦‚æœæä¾›äº†å½“å‰åŸŸï¼Œå°è¯•ä»ä¸åŒåŸŸé‡‡æ ·
            if current_domains is not None and len(available_domains) > 1:
                current_domain = current_domains[i].item()
                other_domains = [d for d in available_domains if d != current_domain]
                if other_domains:
                    domain_id = random.choice(other_domains)
                else:
                    domain_id = random.choice(available_domains)
            else:
                domain_id = random.choice(available_domains)
            
            # ä»é€‰å®šåŸŸä¸­éšæœºé‡‡æ ·ä¸€ä¸ªæ ·æœ¬
            domain_samples = self.domain_data_cache[domain_id]
            sample_idx = random.randint(0, len(domain_samples) - 1)
            style_samples.append(domain_samples[sample_idx])
        
        return torch.stack(style_samples)
        
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_mae = 0
        total = 0
        nan_batches = 0
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"è®­ç»ƒ{self.model_name}", leave=False)
        
        for batch_idx, batch in enumerate(pbar):
            data = batch['data'].float().to(self.device)
            labels = batch['rtime'].float().to(self.device)
            domains = batch['domain'] if 'domain' in batch else None
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®
            if torch.isnan(data).any() or torch.isnan(labels).any():
                self.logger.warning(f"æ‰¹æ¬¡ {batch_idx} è¾“å…¥æ•°æ®åŒ…å«NaNå€¼ï¼Œè·³è¿‡")
                continue
            
            if torch.isinf(data).any() or torch.isinf(labels).any():
                self.logger.warning(f"æ‰¹æ¬¡ {batch_idx} è¾“å…¥æ•°æ®åŒ…å«æ— ç©·å€¼ï¼Œè·³è¿‡")
                continue
            
            # è·å–éšæœºåŸŸæ ·æœ¬ä½œä¸ºstyleè¾“å…¥
            style_x = None
            if self.use_adain and random.random() < self.adain_prob:
                style_x = self._get_random_domain_samples(data.size(0), domains)
                if style_x is not None:
                    style_x = style_x.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            if style_x is not None:
                outputs = self.model(data, style_x=style_x).squeeze(-1)
            else:
                outputs = self.model(data).squeeze(-1)
            
            # æ£€æŸ¥æ¨¡å‹è¾“å‡º
            if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                self.logger.warning(f"æ‰¹æ¬¡ {batch_idx} æ¨¡å‹è¾“å‡ºåŒ…å«å¼‚å¸¸å€¼")
                self.logger.warning(f"outputsèŒƒå›´: {outputs.min().item():.4f} - {outputs.max().item():.4f}")
                self.logger.warning(f"labelsèŒƒå›´: {labels.min().item():.4f} - {labels.max().item():.4f}")
                
                # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦æœ‰å¼‚å¸¸
                param_nan = False
                for name, param in self.model.named_parameters():
                    if torch.isnan(param).any():
                        self.logger.warning(f"å‚æ•° {name} åŒ…å«NaNå€¼")
                        param_nan = True
                        break
                
                if param_nan:
                    self.logger.warning("æ£€æµ‹åˆ°æ¨¡å‹å‚æ•°å¼‚å¸¸ï¼Œé‡æ–°åˆå§‹åŒ–...")
                    self._reinitialize_model()
                    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
                    self.optimizer = optim.Adam(
                        self.model.parameters(), 
                        lr=self.learning_rate, 
                        weight_decay=self.weight_decay
                    )
                
                nan_batches += 1
                continue  # è·³è¿‡è¿™ä¸ªbatch
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, labels)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnan
            if torch.isnan(loss) or torch.isinf(loss):
                self.logger.warning(f"æ‰¹æ¬¡ {batch_idx} æŸå¤±ä¸ºå¼‚å¸¸å€¼: {loss.item()}")
                nan_batches += 1
                continue  # è·³è¿‡è¿™ä¸ªbatch
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰å¼‚å¸¸
            grad_nan = False
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        self.logger.warning(f"å‚æ•° {name} çš„æ¢¯åº¦åŒ…å«å¼‚å¸¸å€¼")
                        grad_nan = True
                        break
            
            if grad_nan:
                self.logger.warning("æ£€æµ‹åˆ°æ¢¯åº¦å¼‚å¸¸ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡")
                self.optimizer.zero_grad()
                continue
            
            # æ·»åŠ æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_loss += loss.item() * labels.size(0)
            total_mae += torch.abs(outputs - labels).sum().item()
            total += labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            if batch_idx % 10 == 0:  # æ¯10ä¸ªbatchæ›´æ–°ä¸€æ¬¡
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'MAE': f'{torch.abs(outputs - labels).mean().item():.4f}'
                })
                
                # è®°å½•è¯¦ç»†çš„æ‰¹æ¬¡ä¿¡æ¯åˆ°æ—¥å¿—
                if batch_idx % 50 == 0:  # æ¯50ä¸ªbatchè®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                    self.logger.debug(f"æ‰¹æ¬¡ {batch_idx}: Loss={loss.item():.6f}, MAE={torch.abs(outputs - labels).mean().item():.6f}")
        
        if total == 0:
            self.logger.error("æ‰€æœ‰æ‰¹æ¬¡éƒ½è¢«è·³è¿‡ï¼Œæ— æ³•è®¡ç®—epochæŸå¤±")
            return float('inf'), float('inf')
        
        epoch_loss = total_loss / total
        epoch_mae = total_mae / total
        
        if nan_batches > 0:
            self.logger.warning(f"æœ¬epochè·³è¿‡äº† {nan_batches} ä¸ªå¼‚å¸¸æ‰¹æ¬¡")
        
        return epoch_loss, epoch_mae
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        total_mae = 0
        total = 0
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"éªŒè¯{self.model_name}", leave=False)
            for batch in pbar:
                data = batch['data'].float().to(self.device)
                labels = batch['rtime'].float().to(self.device)
                domains = batch['domain'] if 'domain' in batch else None
                
                # æ£€æŸ¥è¾“å…¥æ•°æ®
                if torch.isnan(data).any() or torch.isnan(labels).any():
                    continue
                
                if torch.isinf(data).any() or torch.isinf(labels).any():
                    continue
                
                # åœ¨éªŒè¯æ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨AdaINï¼ˆä»¥è¾ƒä½æ¦‚ç‡ï¼‰
                style_x = None
                if self.use_adain and random.random() < self.adain_prob * 0.5:  # éªŒè¯æ—¶ä½¿ç”¨è¾ƒä½æ¦‚ç‡
                    style_x = self._get_random_domain_samples(data.size(0), domains)
                    if style_x is not None:
                        style_x = style_x.to(self.device)
                
                # å‰å‘ä¼ æ’­
                if style_x is not None:
                    outputs = self.model(data, style_x=style_x).squeeze(-1)
                else:
                    outputs = self.model(data).squeeze(-1)
                
                # æ£€æŸ¥è¾“å‡º
                if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                    continue
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(outputs, labels)
                
                # æ£€æŸ¥æŸå¤±
                if torch.isnan(loss) or torch.isinf(loss):
                    continue
                
                # ç»Ÿè®¡ä¿¡æ¯
                total_loss += loss.item() * labels.size(0)
                total_mae += torch.abs(outputs - labels).sum().item()
                total += labels.size(0)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'MAE': f'{torch.abs(outputs - labels).mean().item():.4f}'
                })
        
        if total == 0:
            return float('inf'), float('inf')
        
        epoch_loss = total_loss / total
        epoch_mae = total_mae / total
        
        return epoch_loss, epoch_mae
    
    def train(self, train_loader, val_loader, save_path=None):
        """è®­ç»ƒæ¨¡å‹"""
        best_val_loss = float('inf')
        best_val_mae = float('inf')
        patience_counter = 0
        
        self.logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {self.model_name}")
        self.logger.info(f"è®­ç»ƒè®¾å¤‡: {self.device}")
        self.logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        self.logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_loader.dataset)}")
        self.logger.info(f"éªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        self.logger.info(f"å­¦ä¹ ç‡: {self.learning_rate}")
        self.logger.info(f"æƒé‡è¡°å‡: {self.weight_decay}")
        
        # æ„å»ºåŸŸæ•°æ®ç¼“å­˜ç”¨äºAdaIN
        self._build_domain_cache(train_loader)
        
        print(f"\n" + "="*80)
        print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {self.model_name}")
        print("="*80)
        
        for epoch in range(self.num_epochs):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“Š Epoch {epoch+1}/{self.num_epochs}")
            print("-" * 50)
            
            # è®­ç»ƒ
            train_loss, train_mae = self.train_epoch(train_loader)
            
            # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œæå‰ç»“æŸ
            if train_loss == float('inf'):
                self.logger.error("è®­ç»ƒå¤±è´¥ï¼Œæå‰ç»“æŸ")
                break
            
            # éªŒè¯
            val_loss, val_mae = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            old_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step(val_loss)
            new_lr = self.optimizer.param_groups[0]['lr']
            
            # å­˜å‚¨å†å²è®°å½•
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            epoch_time = time.time() - epoch_start_time
            
            # è®°å½•epochç»“æœåˆ°æ—¥å¿—
            self.logger.info(f"Epoch {epoch+1}/{self.num_epochs} å®Œæˆ:")
            self.logger.info(f"  è®­ç»ƒæŸå¤±(MSE): {train_loss:.6f}")
            self.logger.info(f"  è®­ç»ƒMAE: {train_mae:.6f}")
            self.logger.info(f"  éªŒè¯æŸå¤±(MSE): {val_loss:.6f}")
            self.logger.info(f"  éªŒè¯MAE: {val_mae:.6f}")
            self.logger.info(f"  å­¦ä¹ ç‡: {new_lr:.8f}")
            self.logger.info(f"  Epochæ—¶é—´: {epoch_time:.2f}ç§’")
            
            # æ‰“å°è¯¦ç»†çš„è®­ç»ƒä¿¡æ¯åˆ°æ§åˆ¶å°
            print(f"ğŸš€ è®­ç»ƒç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±(MSE): {train_loss:.6f}")
            print(f"   è®­ç»ƒMAE:      {train_mae:.6f}")
            print(f"ğŸ¯ éªŒè¯ç»“æœ:")
            print(f"   éªŒè¯æŸå¤±(MSE): {val_loss:.6f}")
            print(f"   éªŒè¯MAE:      {val_mae:.6f}")
            print(f"âš™ï¸  è®­ç»ƒå‚æ•°:")
            print(f"   å­¦ä¹ ç‡:       {new_lr:.8f}")
            if old_lr != new_lr:
                print(f"   å­¦ä¹ ç‡å·²è°ƒæ•´: {old_lr:.8f} â†’ {new_lr:.8f}")
                self.logger.info(f"å­¦ä¹ ç‡å·²è°ƒæ•´: {old_lr:.8f} â†’ {new_lr:.8f}")
            print(f"   Epochæ—¶é—´:    {epoch_time:.2f}ç§’")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_val_mae = val_mae
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_path:
                    self.save_model(save_path)
                    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
                    self.logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
                    
                print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
                self.logger.info(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
            else:
                patience_counter += 1
                print(f"â° æ—©åœè®¡æ•°å™¨: {patience_counter}/{self.early_stopping_patience}")
                self.logger.info(f"æ—©åœè®¡æ•°å™¨: {patience_counter}/{self.early_stopping_patience}")
                
            if patience_counter >= self.early_stopping_patience:
                print(f"\nâ¹ï¸  åœ¨ç¬¬ {epoch+1} ä¸ªepochåè§¦å‘æ—©åœ")
                self.logger.info(f"åœ¨ç¬¬ {epoch+1} ä¸ªepochåè§¦å‘æ—©åœ")
                break
        
        print(f"\n" + "="*80)
        print(f"æ¨¡å‹ {self.model_name} è®­ç»ƒå®Œæˆ")
        print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"ğŸ† æœ€ä½³éªŒè¯MAE:  {best_val_mae:.6f}")
        print("="*80)
        
        # è®°å½•æœ€ç»ˆç»“æœ
        self.logger.info(f"æ¨¡å‹ {self.model_name} è®­ç»ƒå®Œæˆ")
        self.logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        self.logger.info(f"æœ€ä½³éªŒè¯MAE: {best_val_mae:.6f}")
        self.logger.info(f"è®­ç»ƒæ€»å…±è¿›è¡Œäº† {len(self.train_losses)} ä¸ªepoch")
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': best_val_loss,
            'best_val_mae': best_val_mae
        }
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config,
            'train_history': {
                'train_losses': self.train_losses,
                'val_losses': self.val_losses
            }
        }, path)
        self.logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤è®­ç»ƒå†å²
        history = checkpoint.get('train_history', {})
        self.train_losses = history.get('train_losses', [])
        self.val_losses = history.get('val_losses', [])
        
        self.logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä» {path} åŠ è½½")
        
        return checkpoint.get('config', {})